{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from missforest import MissForest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_l1_ball(v, s=1):\n",
    "    \"\"\" Compute the Euclidean projection on a L1-ball.\n",
    "\n",
    "    Solves the optimization problem (using the algorithm from [Duchi et al. 2008]):\n",
    "        min_w 0.5 * ||w - v||^2 s.t. ||w||_1 <= s\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    v : (n,) numpy array\n",
    "        input vector\n",
    "    s : float, optional\n",
    "        radius of the L1-ball (default: 1)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w : (n,) numpy array\n",
    "        Euclidean projection of v on the L1-ball of radius s\n",
    "    \"\"\"\n",
    "    v = np.array(v)\n",
    "    if np.linalg.norm(v, 1) <= s:\n",
    "        # Already within the L1-ball\n",
    "        return v\n",
    "\n",
    "    u = np.abs(v)\n",
    "    if u.sum() <= s:\n",
    "        return v\n",
    "\n",
    "    # Sort the components of u in descending order\n",
    "    u_sort = np.sort(u)[::-1]\n",
    "    cssv = np.cumsum(u_sort) - s\n",
    "    ind = np.arange(len(v)) + 1\n",
    "    cond = u_sort - cssv / ind > 0\n",
    "    rho = ind[cond].max()\n",
    "    theta = cssv[rho - 1] / rho\n",
    "\n",
    "    # Project v onto the L1-ball\n",
    "    w = np.sign(v) * np.maximum(u - theta, 0)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2*10**4\n",
    "d = 50\n",
    "X = np.random.uniform(0, 1, (n, d))\n",
    "epsilon = np.random.normal(0, 0.25, n)\n",
    "Y = np.zeros(n)\n",
    "Y_true = np.zeros(n)\n",
    "Omega = np.random.binomial(1, 0.5, (n, d))\n",
    "# for i in range(n):\n",
    "#     if X[i,1] <= 0.5:\n",
    "#         Omega[i,1] = 1\n",
    "#     else:\n",
    "#         Omega[i,1] = 0\n",
    "sample_mean = np.sum(X*Omega, axis = 0) / np.sum(Omega, axis = 0)\n",
    "Z_ZI = X * Omega\n",
    "Z_MI = X * Omega + sample_mean * (1 - Omega)\n",
    "Z_RI = X * Omega + (1 - Omega) * np.random.uniform(0, 1, (n, d))\n",
    "\n",
    "def reg_func(x):\n",
    "    # out = (4 * x[1] - 2)**2 + 2 * np.sin(np.pi * x[2]) * (np.sqrt(x[3]) + 1) + 6 * np.abs(x[3] - 0.5)\n",
    "    out = 2 * x[1]**(1/3) + np.exp((x[2] + x[3])/2) + (4 * x[3] - 2)**2   # Bayes risk = 0.8642027\n",
    "    # out = 2 * x[1]**(1/3) + (4 * x[2] - 2)**2\n",
    "    return out\n",
    "for i in range(n):\n",
    "    Y_true[i] = reg_func(X[i,:])\n",
    "    Y[i] = Y_true[i] + epsilon[i]\n",
    "\n",
    "\n",
    "Z_ZI_train = Z_ZI[0:int(n/2), :]\n",
    "Z_ZI_test = Z_ZI[int(n/2):n, :]\n",
    "Z_MI_train = Z_MI[0:int(n/2), :]\n",
    "Z_MI_test = Z_MI[int(n/2):n, :]\n",
    "Omega_train = Omega[0:int(n/2), :]\n",
    "Omega_test = Omega[int(n/2):n, :]\n",
    "\n",
    "Y_train = Y[0:int(n/2)]\n",
    "Y_test = Y[int(n/2):n]\n",
    "Y_true_test = Y_true[int(n/2):n]\n",
    "\n",
    "Z_ZI_train = torch.tensor(Z_ZI_train, dtype=torch.float32)\n",
    "Z_ZI_test = torch.tensor(Z_ZI_test, dtype=torch.float32)\n",
    "Z_MI_train = torch.tensor(Z_MI_train, dtype=torch.float32)\n",
    "Z_MI_test = torch.tensor(Z_MI_test, dtype=torch.float32)\n",
    "Omega_train = torch.tensor(Omega_train, dtype=torch.float32)\n",
    "Omega_test = torch.tensor(Omega_test, dtype=torch.float32)\n",
    "Z_Omega_train = torch.cat((Z_ZI_train, Omega_train), dim = 1)\n",
    "Z_Omega_test = torch.cat((Z_ZI_test, Omega_test), dim = 1)\n",
    "\n",
    "Y_train = torch.tensor(Y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test.reshape(-1, 1), dtype=torch.float32)\n",
    "Y_true_test = torch.tensor(Y_true_test.reshape(-1, 1), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.9426525831222534\n",
      "Epoch 19, Loss: 1.3378350734710693\n",
      "Epoch 29, Loss: 1.0007822513580322\n",
      "Epoch 39, Loss: 0.9499924778938293\n",
      "Epoch 49, Loss: 1.3707633018493652\n",
      "Epoch 59, Loss: 0.8427971601486206\n",
      "0.05933386\n"
     ]
    }
   ],
   "source": [
    "### pattern augmented NN\n",
    "class PANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.arch1 = nn.Sequential(\n",
    "            nn.Linear(2*d, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.arch1(x)\n",
    "        return out\n",
    "    \n",
    "model_PA = PANN()\n",
    "PA_train_data = TensorDataset(Z_Omega_train, Y_train)\n",
    "PA_train_loader = DataLoader(dataset = PA_train_data, batch_size=20, shuffle=True)\n",
    "\n",
    "lr = 0.01\n",
    "l1_lambda = 0.003\n",
    "epochs = 60\n",
    "\n",
    "optimizer = optim.SGD(model_PA.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for x_batch, y_batch in PA_train_loader:\n",
    "        y_batch = y_batch.view(-1, 1)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model_PA(x_batch)\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "\n",
    "        # L1 penalty\n",
    "        l1_penalty = 0\n",
    "        for param in model_PA.parameters():\n",
    "            l1_penalty += torch.sum(torch.abs(param))\n",
    "        # Add L1 penalty to the loss\n",
    "        loss += l1_lambda * l1_penalty\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "ER_PA = loss_fn(model_PA(Z_Omega_test), Y_true_test) - 0.8642027\n",
    "print(ER_PA.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 1.7265633344650269\n",
      "Epoch 19, Loss: 0.9484437704086304\n",
      "Epoch 29, Loss: 1.2485803365707397\n",
      "Epoch 39, Loss: 2.695195198059082\n",
      "Epoch 49, Loss: 1.4081701040267944\n",
      "Epoch 59, Loss: 1.0744986534118652\n",
      "0.21589214\n"
     ]
    }
   ],
   "source": [
    "### mean imputation\n",
    "class MINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.arch1 = nn.Sequential(\n",
    "            nn.Linear(d, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.arch1(x)\n",
    "        return out\n",
    "    \n",
    "model_MI = MINN()\n",
    "train_data = TensorDataset(Z_MI_train, Y_train)\n",
    "train_loader = DataLoader(dataset = train_data, batch_size=20, shuffle=True)\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 60\n",
    "l1_lambda = 0.002\n",
    "\n",
    "optimizer = optim.SGD(model_MI.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        y_batch = y_batch.view(-1, 1)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model_MI(x_batch)\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "\n",
    "        # L1 penalty\n",
    "        l1_penalty = 0\n",
    "        for param in model_MI.parameters():\n",
    "            l1_penalty += torch.sum(torch.abs(param))\n",
    "        # Add L1 penalty to the loss\n",
    "        loss += l1_lambda * l1_penalty\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "ER_MI = loss_fn(model_MI(Z_MI_test), Y_true_test) - 0.8642027\n",
    "print(ER_MI.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:02<00:03,  1.32s/it]\n",
      " 40%|████      | 2/5 [00:02<00:03,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "### MissForest imputation\n",
    "Z_nan = np.copy(Z_ZI)\n",
    "for i in range(n):\n",
    "    for j in range(d):\n",
    "        if Omega[i, j] == 0:\n",
    "            Z_nan[i,j] = np.nan\n",
    "Z_nan_train = pd.DataFrame(Z_nan[0:int(n/2), :])\n",
    "Z_nan_test = pd.DataFrame(Z_nan[int(n/2):n, :])\n",
    "rgr = RandomForestRegressor(n_jobs=-1)\n",
    "warnings.filterwarnings('ignore')\n",
    "mf = MissForest(rgr)\n",
    "mf.fit(x=Z_nan_train)\n",
    "Z_MF_train = mf.transform(Z_nan_train)\n",
    "Z_MF_test = mf.transform(Z_nan_test)\n",
    "Z_MF_train = Z_MF_train.to_numpy()\n",
    "Z_MF_test = Z_MF_test.to_numpy()\n",
    "\n",
    "Z_MF_train = torch.tensor(Z_MF_train, dtype=torch.float32)\n",
    "Z_MF_test = torch.tensor(Z_MF_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_PA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# L1 penalty\u001b[39;00m\n\u001b[0;32m     41\u001b[0m l1_penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel_PA\u001b[49m\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[0;32m     43\u001b[0m     l1_penalty \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mabs(param))\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Add L1 penalty to the loss\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_PA' is not defined"
     ]
    }
   ],
   "source": [
    "### MissForest imputation\n",
    "class MFNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.arch1 = nn.Sequential(\n",
    "            nn.Linear(d, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.arch1(x)\n",
    "        return out\n",
    "    \n",
    "model_MF = MFNN()\n",
    "train_data = TensorDataset(Z_MF_train, Y_train)\n",
    "train_loader = DataLoader(dataset = train_data, batch_size=20, shuffle=True)\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 60\n",
    "l1_lambda = 0.002\n",
    "\n",
    "optimizer = optim.SGD(model_MF.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        y_batch = y_batch.view(-1, 1)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model_MF(x_batch)\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "\n",
    "        # L1 penalty\n",
    "        l1_penalty = 0\n",
    "        for param in model_MF.parameters():\n",
    "            l1_penalty += torch.sum(torch.abs(param))\n",
    "        # Add L1 penalty to the loss\n",
    "        loss += l1_lambda * l1_penalty\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "ER_MF = loss_fn(model_MF(Z_MF_test), Y_true_test) - 0.8642027\n",
    "print(ER_MF.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
