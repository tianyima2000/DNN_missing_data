{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from missforest import MissForest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43405, 65)\n",
      "   year        A1       A2       A3      A4       A5       A6        A7  \\\n",
      "0     1  0.200550  0.37951  0.39641  2.0472  32.3510  0.38825  0.249760   \n",
      "1     1  0.209120  0.49988  0.47225  1.9447  14.7860  0.00000  0.258340   \n",
      "2     1  0.248660  0.69592  0.26713  1.5548  -1.1523  0.00000  0.309060   \n",
      "3     1  0.081483  0.30734  0.45879  2.4928  51.9520  0.14988  0.092704   \n",
      "4     1  0.187320  0.61323  0.22960  1.4063  -7.3128  0.18732  0.187320   \n",
      "\n",
      "        A8      A9  ...       A55       A56      A57      A58       A59  \\\n",
      "0  1.33050  1.1389  ...  348690.0  0.121960  0.39718  0.87804  0.001924   \n",
      "1  0.99601  1.6996  ...    2304.6  0.121300  0.42002  0.85300  0.000000   \n",
      "2  0.43695  1.3090  ...    6332.7  0.241140  0.81774  0.76599  0.694840   \n",
      "3  1.86610  1.0571  ...   20545.0  0.054015  0.14207  0.94598  0.000000   \n",
      "4  0.63070  1.1559  ...    3186.6  0.134850  0.48431  0.86515  0.124440   \n",
      "\n",
      "      A60     A61      A62     A63      A64  \n",
      "0  8.4160  5.1372   82.658  4.4158   7.4277  \n",
      "1  4.1486  3.2732  107.350  3.4000  60.9870  \n",
      "2  4.9909  3.9510  134.270  2.7185   5.2078  \n",
      "3  4.5746  3.6147   86.435  4.2228   5.5497  \n",
      "4  6.3985  4.3158  127.210  2.8692   7.8980  \n",
      "\n",
      "[5 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "polish_companies_bankruptcy = fetch_ucirepo(id=365) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = polish_companies_bankruptcy.data.features \n",
    "Y = polish_companies_bankruptcy.data.targets \n",
    "  \n",
    "# # metadata \n",
    "# print(polish_companies_bankruptcy.metadata) \n",
    "  \n",
    "# # variable information \n",
    "# print(polish_companies_bankruptcy.variables) \n",
    "\n",
    "# data dimension\n",
    "print(X.shape)\n",
    "print(X.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[:,'year'] = X.loc[:,'year'].astype('category')\n",
    "X = pd.get_dummies(X, columns=['year'], prefix='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Omega = X.isna().to_numpy()\n",
    "Omega = Omega.astype(int)\n",
    "\n",
    "X_ZI = X.copy(deep=True)\n",
    "X_ZI = X_ZI.fillna(0)\n",
    "X_ZI = X_ZI.to_numpy()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_ZI)\n",
    "X_ZI = scaler.transform(X_ZI)\n",
    "Y = Y.to_numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ZI_train, X_ZI_test, Y_train, Y_test, Omega_train, Omega_test = train_test_split(X_ZI, Y, Omega, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ZI_train = torch.tensor(X_ZI_train, dtype=torch.float32)\n",
    "X_ZI_test = torch.tensor(X_ZI_test, dtype=torch.float32)\n",
    "\n",
    "Omega_train = torch.tensor(Omega_train, dtype=torch.float32)\n",
    "Omega_test = torch.tensor(Omega_test, dtype=torch.float32)\n",
    "\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.08916226029396057\n",
      "Epoch 19, Loss: 0.07397502660751343\n",
      "Epoch 29, Loss: 0.10288068652153015\n",
      "Epoch 39, Loss: 0.11827834695577621\n",
      "Epoch 49, Loss: 0.021093865856528282\n",
      "Epoch 59, Loss: 0.0789480060338974\n",
      "Epoch 69, Loss: 0.08012749254703522\n",
      "Epoch 79, Loss: 0.06209595501422882\n",
      "Epoch 89, Loss: 0.11358650773763657\n",
      "Epoch 99, Loss: 0.04667492210865021\n",
      "Epoch 109, Loss: 0.012480970472097397\n",
      "Epoch 119, Loss: 0.007120020687580109\n",
      "Epoch 129, Loss: 0.06877294182777405\n",
      "Epoch 139, Loss: 0.04933222010731697\n",
      "Epoch 149, Loss: 0.04728896543383598\n",
      "Epoch 159, Loss: 0.02614790014922619\n",
      "Epoch 169, Loss: 0.1637321412563324\n",
      "Epoch 179, Loss: 0.10963525623083115\n",
      "Epoch 189, Loss: 0.06982632726430893\n",
      "Epoch 199, Loss: 0.019491545855998993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.03985715931344315)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.001\n",
    "epochs = 200\n",
    "l1_lambda = 0\n",
    "\n",
    "class TwoStreamModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        embedding_dim = 5\n",
    "        \n",
    "        # The embedding layer\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(69, 20),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, embedding_dim),  \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Combined network for the layers after embedding\n",
    "        self.combined = nn.Sequential(\n",
    "            nn.Linear(69 + embedding_dim, 140),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(140, 140),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(140, 70),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(70, 70),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(70, 70),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(70, 2)  # Output layer\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, omega):\n",
    "        # Apply the embedding layer to omega\n",
    "        embedded = self.embedding(omega)\n",
    "        \n",
    "        # Concatenate the embedding with x\n",
    "        combined_features = torch.cat((x, embedded), dim=1)\n",
    "        \n",
    "        # Apply the combined network\n",
    "        final_out = self.combined(combined_features)\n",
    "        \n",
    "        return final_out\n",
    "    \n",
    "\n",
    "model_PE = TwoStreamModel()\n",
    "PA_train_data = TensorDataset(X_ZI_train, Omega_train, Y_train)\n",
    "PA_train_loader = DataLoader(dataset = PA_train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "optimizer = optim.Adam(model_PE.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "class_weights = torch.tensor([0.05, 0.95], dtype=torch.float32)\n",
    "loss_fn = nn.CrossEntropyLoss()    # (weight=class_weights)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for x_batch, omega_batch, y_batch in PA_train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model_PE(x_batch, omega_batch)\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "\n",
    "        # L1 penalty\n",
    "        l1_penalty = 0\n",
    "        for param in model_PE.parameters():\n",
    "            l1_penalty += torch.sum(torch.abs(param))\n",
    "        # Add L1 penalty to the loss\n",
    "        loss += l1_lambda * l1_penalty\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "_, labels = torch.max(model_PE(X_ZI_test, Omega_test), dim=1)\n",
    "pred = labels.detach().numpy()\n",
    "np.mean(np.abs(pred - Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017624697615482087\n",
      "1094\n"
     ]
    }
   ],
   "source": [
    "_, labels2 = torch.max(model_PE(X_ZI_train, Omega_train), dim=1)\n",
    "pred2 = labels2.detach().numpy()\n",
    "print(np.mean(np.abs(pred2 - Y_train.detach().numpy())))\n",
    "print(sum(pred2 == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
